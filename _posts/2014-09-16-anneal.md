---
layout: post 
category : technology
title: Randomly walking while simulated annealing 
matheq: yes
comments: yes
tags : [simulated annealing, optimizing, tired]

---

A good SA(simulated annealing) [example](http://apmonitor.com/me575/index.php/Main/SimulatedAnnealing) provide both code and procedure illustration, which is cognitively straightforward and possibly the best SA example "googled". 
One critical step of this SA after assertion `current design` is to `randomly generate design`, and procedure after that is "to select better" essentially.
![flow diagram](http://apmonitor.com/me575/uploads/Main/sim_annealing_flow.png "flow diagram")
If the searching space is derivable in every searching dimension, the final step of SA needs fine tuning while current step located in range of \\(\\Delta E\\), \\(\\Delta E \\le \\epsilon\\). 
Here \\(\\epsilon > 0\\), and \\(\\epsilon\\)should be very close to \\(0\\) to make SA reach global minimum.
While current state is close to global minimum, it is no need to have the same probability to take a big step while generating the candidate design and current state is quite far away from global minimum.

The original SA generate candidate design with unchangeable step range `random.random() - 0.5` (\\(\\pm 0.5\\)):
{% highlight python  %}
xi[0] = xc[0] + random.random() - 0.5
{% endhighlight %}

SA path of algorithm with unchangeable step range probability:
![SA](http://apmonitor.com/me575/uploads/Main/sim_annealing_contour.png "SA")

Here if we make the step range proportional to temperature:
{% highlight python  %}
xi[0] = xc[0] + ( random.random() - 0.5 ) * (t / self.t1)
{% endhighlight %}

Searching path shrinks much more closer to global minimum: 
![SA+](https://2s66lw.bl3301.livefilestore.com/y2pmIN_ioDbtUuthMFjFJH8GB9leYESHSdEmrvhFHoJw2Bq3xwEr5OI_IVw72TuTlQthS-fN1wuSQiqL1KbAgB4-nN9-_v20FHkorl-2pT-OCY/contour_steprange.png "SA+")
 
Test these two algorithm 100 times respectively, record every value of the objective function and location for each step, the latter are more closer to global minimum generally but more widely scattered.
![boxplot](https://2s66lw.bl3301.livefilestore.com/y2puflsqWic1OLyHK5HKDX-fF30ds1UT_c2eJI_P2o1EUCWmAkmucQXXTny7eZ-besbC8M_jWQwDSpcs5ET_kQ_8vfvNYAZlKsiK3VRP9mSr3o/obj_com.png "boxplot")

The final location are much more closer to global minimum \\((0,0)\\) (red), while the original SA has less outliers (blue): 
![boxplotx](https://2s66lw.bl3301.livefilestore.com/y2pq94GywcmtEsuoWCPClC4MPTlzlEuk-hq2uZY20V2EF4H29-bGTf2_2a1cqeGgM1PJgAQll0PaRYtcbDrQSrrgVnBcfiwurRsSrYNot7BBfw/x1_com.png "boxplotx")

Well, here comes the trade-off between profit and risk.
If we want high reward, we better prepare to undertake this:
![risk](https://2s66lw.bl3301.livefilestore.com/y2pROXDF0XMduUj_O54vw3DXmRD-rHwnbt0E-siSA6fUJXxk6E2WGzTpWU-IYLzS1lCKgZHB8xUlfjaJg5k7tcWMX_oMYV5KH4iTfhUjvJPaPc/contourrisk.png "risk")

To be more generalized, we could modify the probability distribution further more, like, normal distribution(scales with temperature):
{% highlight python  %}
xi[0] = xc[0] + np.random.normal(0 , 0.3*t/self.t1, 1)[0]
{% endhighlight %}

